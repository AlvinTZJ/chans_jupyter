{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with MNIST dataset\n",
    "> In this post, we will implement various type of CNN for MNIST dataset. In Tensorflow, there are various ways to define CNN model like sequential model, functional model, and sub-class model. We'll simply implement each type and test it.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Chanseok Kang\n",
    "- categories: [Python, Deep_Learning, Tensorflow-Keras]\n",
    "- image: images/CNN_MNIST.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "plt.rc('font', size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN model with sequential API\n",
    "Previously, we learned basic operation of convolution and max-pooling. Actually, we already implemented simple type of CNN model for MNIST classification, which is manually combined with 2D convolution layer and max-pooling layer. But there are other ways to define CNN model. In this section, we will implement CNN model with Sequential API.\n",
    "\n",
    "Briefly speaking, we will build the model as follows,\n",
    "\n",
    "![CNN](image/CNN_MNIST.png)\n",
    "\n",
    " 3x3 2D convolution layer is defined as an input layer, and post-process with 2x2 max-pooling. And these process will be redundant 3 times, then set fully-connected layer as an output layer for classification. In convolution layer, stride will be 1, and padding will be `same` (that is, we will use half padding). And in max-pooling layer, stride will be 2, and padding will also be `same`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter setting\n",
    "Firstly, we need to define hyperparameter that affect model training. For the review, **hyperparameter** is a parameter whose value is used to control the learning process, such as learning rate, epochs, and batch_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for the tracking model training, it is helpful to build checkpoint while training the model, so when we the model training is failed due to unexpected reason, we can re-train it with checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cur_dir = os.getcwd()\n",
    "checkpoint_dir = os.path.join(cur_dir, 'checkpoints', 'mnist_cnn_seq')\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'mnist_cnn_seq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pipelining\n",
    "Before model implementation, it requires data pipelining, also known as data-preprocess. As you can see from previous example, the original raw data is hardly used directly. So we need to normalize it, convert it, that we can express whole process as an \"data-preprocessing\".\n",
    "\n",
    "Note that, the label of each data is class label. So to use it in Neural network model, it needs to encode it as an binary code. Maybe someone already knew it, it is **one-hot** encoding. Luckily, tf.keras also implements `to_categorical` for one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalization\n",
    "X_train = X_train.astype(np.float32) / 255.\n",
    "X_test = X_test.astype(np.float32) / 255.\n",
    "\n",
    "# Convert it to 4D array (or we can use np.expand_dims for dimension expansion)\n",
    "X_train = X_train[..., tf.newaxis]\n",
    "X_test = X_test[..., tf.newaxis]\n",
    "\n",
    "# one-hot encoding\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Build dataset pipeline\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(buffer_size=100000).batch(batch_size)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model with Sequential API\n",
    "Building model with Sequential API is similar with previous example. The difference is that Sequential API pre-build the model skeleton, then add each specific layers. In this code, we will build one API to build whole models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CNN_Sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               524544    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 619,786\n",
      "Trainable params: 619,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.Sequential(name='CNN_Sequential')\n",
    "    model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation=tf.keras.activations.relu,\n",
    "                                     padding='SAME', input_shape=(28, 28, 1)))\n",
    "    model.add(tf.keras.layers.MaxPool2D(padding='SAME'))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation=tf.keras.activations.relu,\n",
    "                                     padding='SAME'))\n",
    "    model.add(tf.keras.layers.MaxPool2D(padding='SAME'))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation=tf.keras.activations.relu,\n",
    "                                     padding='SAME'))\n",
    "    model.add(tf.keras.layers.MaxPool2D(padding='SAME'))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(256, activation=tf.keras.activations.relu))\n",
    "    model.add(tf.keras.layers.Dropout(0.4))\n",
    "    model.add(tf.keras.layers.Dense(10))\n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, when we directly add the layer, we need to enter the input data for generating output. But in Sequential model, each previous layers node is connected with next layers node automatically, All we need to do is to input the data in the model, then output will be generated from the whole model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function and Gradient \n",
    "Same as MLP, we need to define loss function and use gradient descent for finding minimum loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def loss_fn(model, images, labels):\n",
    "    logits = model(images, training=True)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    return loss\n",
    "\n",
    "# Gradient Function\n",
    "def grad(model, images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_fn(model, images, labels)\n",
    "    return tape.gradient(loss, model.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and Evaluation\n",
    "For finding optimum value, we will use \"Adam\" Optimizer with predifined learning_rate. Also, we need to define evaluation function so that we can check the performance (or accuracy of model).\n",
    "\n",
    "One more thing, We already mention that checkpoint is required for tracking history. So we will define it here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, images, labels):\n",
    "    logits = model(images, training=False)\n",
    "    correct_predict = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predict, tf.float32))\n",
    "    return accuracy\n",
    "\n",
    "# Checkpoint\n",
    "checkpoint = tf.train.Checkpoint(cnn=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation\n",
    "Finally, we can train model with our training dataset. And also we need to check the performance while training the model, so after train the model in each epoch, we will also evaluate the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 0.04735789 train acc: 0.9899 test acc: 0.9892\n",
      "Epoch: 2 loss: 0.03165784 train acc: 0.9932 test acc: 0.9881\n",
      "Epoch: 3 loss: 0.02270938 train acc: 0.9953 test acc: 0.9937\n",
      "Epoch: 4 loss: 0.01783992 train acc: 0.9968 test acc: 0.9917\n",
      "Epoch: 5 loss: 0.01486052 train acc: 0.9973 test acc: 0.9921\n",
      "Epoch: 6 loss: 0.01309034 train acc: 0.9980 test acc: 0.9938\n",
      "Epoch: 7 loss: 0.01007799 train acc: 0.9983 test acc: 0.9931\n",
      "Epoch: 8 loss: 0.00882571 train acc: 0.9986 test acc: 0.9912\n",
      "Epoch: 9 loss: 0.00827104 train acc: 0.9988 test acc: 0.9939\n",
      "Epoch: 10 loss: 0.00748011 train acc: 0.9991 test acc: 0.9932\n",
      "Epoch: 11 loss: 0.00597106 train acc: 0.9991 test acc: 0.9924\n",
      "Epoch: 12 loss: 0.00593605 train acc: 0.9993 test acc: 0.9938\n",
      "Epoch: 13 loss: 0.00504262 train acc: 0.9995 test acc: 0.9937\n",
      "Epoch: 14 loss: 0.00443290 train acc: 0.9995 test acc: 0.9925\n",
      "Epoch: 15 loss: 0.00462466 train acc: 0.9996 test acc: 0.9934\n"
     ]
    }
   ],
   "source": [
    "for e in range(training_epochs):\n",
    "    avg_loss = 0.\n",
    "    avg_train_acc = 0.\n",
    "    avg_test_acc = 0.\n",
    "    train_step = 0\n",
    "    test_step = 0\n",
    "    \n",
    "    for images, labels in train_ds:\n",
    "        grads = grad(model, images, labels)\n",
    "        optimizer.apply_gradients(zip(grads, model.variables))\n",
    "        loss = loss_fn(model, images, labels)\n",
    "        acc = evaluate(model, images, labels)\n",
    "        avg_loss = avg_loss + loss\n",
    "        avg_train_acc = avg_train_acc + acc\n",
    "        train_step += 1\n",
    "    avg_loss = avg_loss / train_step\n",
    "    avg_train_acc = avg_train_acc / train_step\n",
    "    \n",
    "    for images, labels in test_ds:\n",
    "        acc = evaluate(model, images, labels)\n",
    "        avg_test_acc = avg_test_acc + acc\n",
    "        test_step += 1\n",
    "    avg_test_acc = avg_test_acc / test_step\n",
    "    \n",
    "    print(\"Epoch: {}\".format(e + 1),\n",
    "          \"loss: {:.8f}\".format(avg_loss),\n",
    "          \"train acc: {:.4f}\".format(avg_train_acc),\n",
    "          \"test acc: {:.4f}\".format(avg_test_acc))\n",
    "    \n",
    "    checkpoint.save(file_prefix=checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find that it works in Sequential API. Now let's implement it with another approach, the Functional APIs\n",
    "\n",
    "WIP."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
