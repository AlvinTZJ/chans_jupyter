{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Basic\n",
    "> In this post, We will dig into the basic operation of Convolutional Neural Network, and explain about what each layer look like. And we will simply implement the basic CNN archiecture with tensorflow.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Chanseok Kang\n",
    "- categories: [Python, Deep_Learning, Tensorflow-Keras]\n",
    "- image: images/cnn_stacked.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "plt.rc('font', size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network\n",
    " [Convolutional Neural Network](https://en.wikipedia.org/wiki/Convolutional_neural_network) (CNN for short) is the most widely used for image classification. Previously, we handled image classification problem (Fashion-MNIST) with Multi Layer Perceptron, and we found that works. At that time, the information containing each image is very simple, and definitely classified with several parameters. But if the dataset contains high resolution image and several channels, it may require huge amount of parameters with mlp in image classification. How can we effectively classify the image without huge amount of parameters? And that's why the CNN exists.\n",
    " \n",
    "![CNN example](image/cnn_example.png)\n",
    "*Fig 1. Example of Convolutional Neural Network*\n",
    " \n",
    " Generally, it consists of **Convolution layer**, **Pooling layer**, and **Fully-connected layer**. Usually, Convolution layer and Pooling layer is used for feature extraction. Feature extraction means that it extract important features from image for classification, so that all pixels in images is not required. Through these layer, information of data is resampled, and refined, and go to the output node. Fully-connected (also known as FC) is used for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D Convolution Layer\n",
    "To understand convolution layer, we need to see the shape of image and some elements to extract features called filters(also called kernel).\n",
    "\n",
    "![convolution layer](image/conv_layer.png)\n",
    "*Fig 2. Image shape and filters*\n",
    "\n",
    "In Fashion-MNIST example, the image containing dataset is grayscale. So the range of pixel value is from 0 to 255. But most of real-world image and all the things we see now is colorized. Technically, it can expressed with 3 base color, Red, Green, Blue. The color system which can express these 3 colors are called **RGB**, and each range of pixel value is also from 0 to 255, same as grayscaled pixel.\n",
    "\n",
    "> Note: There are other color systems like RGBA (Red-Green-Blue-Alpha) or HSV (Hue-Saturation-Value), HSL (Hue-Saturation-Lightness), etc. In this post, we just assume that image has 3 channels (RGB)\n",
    "\n",
    "So In the figure, this image has shape of 32x32 and 3 channels (RGB).\n",
    "\n",
    "And Next one is the filter, which shape has 5x5x3. The number we need to focus on **3**. Filter always extend the full channel of the input volume. So it requires to set same number of channels in input nodes and filters. \n",
    "\n",
    "As you can see, filter has same channel with input image. And let's assume the notation of filter, $w$. Next thing we need to do is just slide th filter over the image spatially, and compute dot products, same as MLP operation (This operation is called **convolution**, and it's the reason why this layer is called Convolution layer). Simply speaking, we just focus on the range covered in filter and dot product like this,\n",
    "\n",
    "$$ w^T x + b $$\n",
    "\n",
    "Of course, we move the filter until it can operate convolution.\n",
    "\n",
    "![animation](image/convolutions.gif)\n",
    "<p>\n",
    "    <em>Fig 3. Animation of convolution operation</em>\n",
    "</p>\n",
    "\n",
    "After that, (Assume we don't apply padding, and stride is 1), we can get output volume which shape has 28x28x1. There is formula to calculate the shape of output volume when the stride and filter is defined,\n",
    "\n",
    "$$ \\frac{(\\text{Height of input} - \\text{Height of filter})}{\\text{Stride}} + 1 $$\n",
    "\n",
    "We can substitute the number in this formula, then we can conclude the height of output volume is 28. The output the process convolution is called **Feature Map** (or Activation Map). One feature map gather from one filter, we can apply several filters in one input image. If we use 6 5x5 filters, we can get 6 separate feature maps. We can stack them into one object, then we get \"new image\" of size 28x28x6.\n",
    "\n",
    "![cnn stacked](image/cnn_stacked.png)\n",
    "<p>\n",
    "    <em>Fig 4. stacked output of convolution</em>\n",
    "</p>\n",
    "\n",
    "Here, one number is need to focus, **6**. 6 means the number of filters we apply in this convolution layer.\n",
    "\n",
    "You may be curious about what filter looks like. Actually, filter was widely used from classic computer vision. For example, to extract the edge of object in image, we can apply the *canny edge filer*, and there is edge detection filter named *sobel filter*, and so on. We can bring the concept of filter in CNN here. In short, filter extract the feature like where the edge is, and convolution layer refine its feature.\n",
    "\n",
    "![operation](image/convolution_op2.png)\n",
    "<p>\n",
    "    <em>Fig 5. Applying filter for each pixel</em>\n",
    "</p>\n",
    "\n",
    "Here, we just consider the one channel of image and apply it with 6 filters. And we can extend it in multi-channel, and lots of filters.\n",
    "\n",
    "![manychannel](image/convolution_manychannel.png)\n",
    "<p>\n",
    "    <em>Fig 6. Convolution operation in many channels and many filters</em>\n",
    "</p>\n",
    "\n",
    "### Options of Convolution\n",
    "\n",
    "So what parameteter may affect operation in convolution layer? We can introduce three main parameters for convolution layer: stride, zero-padding, and activation function.\n",
    "\n",
    "**Stride** is the size of step that how far to go to the right or the bottom to perform the next convolution.  It is important that it defines the output model's size. Review the formula of calculating the feature map size,\n",
    "\n",
    "$$ \\frac{(\\text{Height of input} - \\text{Height of filter})}{\\text{Stride}} + 1 $$\n",
    "\n",
    "We can define any size of strides unless its value is smaller than original model size. As we increase the stride, then the size of feature map will be small, and it means that feature will also be small. Think about that the picture is summarized by just a small dot.\n",
    "\n",
    "**Zero-padding** is another factor that affect the convolution layer. The meaning is contained in its name. A few numbers of zeros surrounds the original image, then process the convolution operation. As we can see from previous process, if the layer is deeper and deeper, then the information will be smaller the original one due to the convolution operation. To preserve the size of original image during process, zero-padding is required. If the filter size is $F$x$F$ and stride is 1, then we can also calculate the zero-padding size,\n",
    "\n",
    "$$\\frac{F - 1}{2} $$\n",
    "\n",
    "You may see the **Activation Function** in MLP. It can also apply in the Convolution layer.\n",
    "For example, when we apply Rectified Linear Unit (ReLU) in feature map, we can remove the value that lower than 0,\n",
    "\n",
    "![cnn_relu](image/cnn_relu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Layer in Tensorflow\n",
    "Let's look at how we can use Convolution layer in tensorflow. Actually, in Tensorflow v2.x, Convolution layer is implemented in keras as an high-level class, so all we need to do is just defineing the parameters correctly. Here is the `__init__` of `tf.keras.layers.Conv2D`.\n",
    "\n",
    "```python\n",
    "tf.keras.layers.Conv2D(\n",
    "    filters, kernel_size, strides=(1, 1), padding='valid', data_format=None,\n",
    "    dilation_rate=(1, 1), groups=1, activation=None, use_bias=True,\n",
    "    kernel_initializer='glorot_uniform', bias_initializer='zeros',\n",
    "    kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
    "    kernel_constraint=None, bias_constraint=None, **kwargs\n",
    ")\n",
    "```\n",
    "\n",
    "We already covered filters, kernel_size (same as filter_size), strides, and so on. Here is brief description of arguments:\n",
    "\n",
    "| arguments |  |\n",
    "| --- | --- |\n",
    "| filters | Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). |\n",
    "| kernel_size | An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. |\n",
    "| strides | An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. |\n",
    "| padding | one of \"valid\" or \"same\" (case-insensitive). |\n",
    "| data_format | A string, one of channels_last (default) or channels_first. The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch_size, height, width, channels) while channels_first corresponds to inputs with shape (batch_size, channels,height, width). It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json. If you never set it, then it will be channels_last. |\n",
    "| activation | Activation function to use. If you don't specify anything, no activation is applied (see [keras.activations](https://www.tensorflow.org/api_docs/python/tf/keras/activations)). |\n",
    "| use_bias | Boolean, whether the layer uses a bias vector. |\n",
    "| kernel_initializer | Initializer for the kernel weights matrix |\n",
    "| bias_initializer | Initializer for the bias vector |\n",
    "\n",
    "We need to focus on the `data_format`. The description said that the default value of data_format is 'channels_last'. It means that all data in this layer must follow in this format order: `(batch, height, width, channels)`\n",
    "\n",
    "And `padding` argument can accept `valid` and `same`. `same` means no zero-padding. In this case, it drops the last convolution if the dimensions do not match (or fraction). If you want to use zero-padding, then we need to define `padding` argument as `same`. In this case, it pads such that feature map size has same as original one.(That's why we call the `same`) Usually it is also called 'half' padding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
